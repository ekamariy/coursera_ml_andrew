{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomaly Detection and Recommender Systems\n",
    "\n",
    "# Introduction\n",
    "\n",
    "First, the anomaly detection algorithm will be implemented and applied to detect failing servers on a network. Next, collaborative filtering will be used to build a recommender system for movies.\n",
    "\n",
    "# Anomaly Detection\n",
    "\n",
    "An anomaly detection algorithm will be implemented to detect anomalous behavior in server computers. The features measure the throughput (mb/s) and latency (ms) of response of each server. While the servers were operating, $m = 307$ examples of how they were behaving were collected, and thus having an unlabeled dataset $\\left\\{x^{(1)},\\dots,x^{(m)}\\right\\}$. It is suspected that the vast majority of these examples are \"normal\" (non-anomalous) examples of the servers operating normally, but there might also be some examples of servers acting anomalously within this dataset.\n",
    "\n",
    "A Gaussian model will be used to detect anomalous examples in the dataset. First, a 2D dataset will allow to visualize what the algorithm is doing. On that dataset a Gaussian distribution will be fitted in order to find values that have very low probability and hence can be considered anomalies. After that, the anomaly detection algorithm will be applied to a larger dataset with many dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load dataset.\n",
    "data1 = loadmat('ex8data1.mat')\n",
    "\n",
    "for key in data1:\n",
    "    print(key)\n",
    "\n",
    "X = data1[\"X\"]\n",
    "Xval = data1[\"Xval\"]\n",
    "yval = data1[\"yval\"].flatten() # shape must be (307,) to use it in \"selectThreshold\".\n",
    "\n",
    "print('\\nVisualizing example dataset for outlier detection.')\n",
    "# Visualize the example dataset.\n",
    "plt.plot(X[:, 0], X[:, 1], 'bx', markersize=3)\n",
    "plt.xlabel('Latency (ms)')\n",
    "plt.ylabel('Throughput (mb/s)')\n",
    "plt.title('Figure 1: The first dataset.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Distribution\n",
    "\n",
    "To perform anomaly detection, first a model will be fitted to the dataâ€™s distribution. Given a training set $\\left\\{x^{(1)},\\dots,x^{(m)}\\right\\} \\left(\\text{ where } x^{(i)} \\in \\mathbb{R}^n \\right)$, the Gaussian distribution for each of the features $x_i$ have to be estimated. For each feature $i = 1 \\dots n$, the parameters $\\mu_i$ and $\\sigma_i^2$ that fit the data in the $i-th$ dimension $\\left\\{x_i^{(1)},\\dots,x_i^{(m)}\\right\\}$ will be found. (the $i-th$ dimension of each example). The Gaussian distribution is given by:\n",
    "\n",
    "$$p\\left(x;\\mu,\\sigma^2\\right)=\\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{\\left(z-\\mu\\right)^2}{2\\sigma^2}},$$\n",
    "\n",
    "where $\\mu$ is the mean and $\\sigma^2$ controls the variance.\n",
    "\n",
    "## Estimating Parameters for a Gaussian\n",
    "\n",
    "The parameters $\\left(\\mu_i, \\sigma_i^2\\right)$ of the $i-th$ feature can be estimated by using the following equations:\n",
    "\n",
    "$$\\mu_i=\\frac{1}{m}\\sum_{j=1}^{m}x_i^{(j)} \\text{ and } \\sigma_i^2=\\frac{1}{m}\\sum_{j=1}^{m}\\left(x_i^{(j)}-\\mu_i\\right)^2$$\n",
    "\n",
    "The code in `estimateGaussian` function will take as input the data matrix $X$ and output an $n$-dimension vector\n",
    "`mu` that holds the mean of all the $n$ features and another $n$-dimension vector `sigma2` that holds the variances of all the features. This can implemented using a for-loop over every feature and every training example (though a vectorized implementation might be more efficient).\n",
    "\n",
    "The contours of the fitted Gaussian distribution are visualized in Figure 2. From the plot, it can be seen that most of the examples are in the region with the highest probability, while the anomalous examples are in the regions with lower probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.linalg as linalg\n",
    "\n",
    "# Create a function to compute the parameters of a Gaussian distribution.\n",
    "def estimateGaussian(X):\n",
    "    \"\"\"\n",
    "    Estimates the parameters (mean, variance) of a\n",
    "    Gaussian distribution using the data in X.\n",
    "    Args:\n",
    "        X     : array(# of training examples m, # of features n)\n",
    "    Returns:\n",
    "        mu    : array(# of features n, 1)\n",
    "        sigma2: array(# of features n, 1)\n",
    "    \"\"\"\n",
    "    # Get useful variables.\n",
    "    m, n = X.shape\n",
    "    \n",
    "    # Init mu and sigma2.\n",
    "    mu = np.zeros((n, 1))\n",
    "    sigma2 = np.zeros((n, 1))\n",
    "    \n",
    "    mu = np.mean(X.T, axis=1)\n",
    "    mu = mu.reshape(mu.shape[0], -1)\n",
    "    sigma2 = np.var(X.T, axis=1)\n",
    "    sigma2 = sigma2.reshape(sigma2.shape[0], -1)\n",
    "\n",
    "    return mu, sigma2\n",
    "\n",
    "# Create a function to compute the probability.\n",
    "def multivariateGaussian(X, mu, Sigma2):\n",
    "    \"\"\"\n",
    "    Computes the probability density function of the examples X\n",
    "    under the multivariate gaussian distribution with parameters\n",
    "    mu and sigma2. If Sigma2 is a matrix, it is treated as the\n",
    "    covariance matrix. If Sigma2 is a vector, it is treated as the\n",
    "    sigma^2 values of the variances in each dimension (a diagonal\n",
    "    covariance matrix).\n",
    "    Args:\n",
    "        X     : array(# of training examples m, # of features n)\n",
    "        mu    : array(# of features n, 1)\n",
    "        Sigma2: array(# of features n, # of features n)\n",
    "    Returns:\n",
    "        p     : array(# of training examples m,)\n",
    "    \"\"\"\n",
    "    k = len(mu)\n",
    "\n",
    "    if (Sigma2.shape[0] == 1) or (sigma2.shape[1] == 1):\n",
    "        Sigma2 = linalg.diagsvd(Sigma2.flatten(),\n",
    "                                len(Sigma2.flatten()),\n",
    "                                len(Sigma2.flatten()))\n",
    "        X = X - mu.T\n",
    "        p = np.dot(np.power(2 * np.pi, - k / 2.0),\n",
    "                   np.power(np.linalg.det(Sigma2), -0.5)) * \\\n",
    "        np.exp(-0.5 * np.sum(np.dot(X, np.linalg.pinv(Sigma2)) * X, axis=1))\n",
    "\n",
    "    return p\n",
    "\n",
    "# Create a function to visualize the dataset and its estimated distribution.\n",
    "def visualizeFit(X, mu, sigma2):\n",
    "    \"\"\"\n",
    "    Visualizes the dataset and its estimated distribution.\n",
    "    This visualization shows the probability density function\n",
    "    of the Gaussian distribution. Each example has a location\n",
    "    (x1, x2) that depends on its feature values.\n",
    "    Args:\n",
    "        X     : array(# of training examples m, # of features n)\n",
    "        mu    : array(# of features n, 1)\n",
    "        sigma2: array(# of features n, 1)\n",
    "    \"\"\"\n",
    "    X1, X2 = np.meshgrid(np.arange(0, 30, 0.5), np.arange(0, 30, 0.5))\n",
    "    Z = multivariateGaussian(np.column_stack((X1.reshape(X1.size),\n",
    "                                              X2.reshape(X2.size))),\n",
    "                             mu, sigma2)\n",
    "    Z = Z.reshape(X1.shape)\n",
    "\n",
    "    plt.plot(X[:, 0], X[:, 1],'bx', markersize=3)\n",
    "\n",
    "    # Do not plot if there are infinities.\n",
    "    if (np.sum(np.isinf(Z)) == 0):\n",
    "        plt.contour(X1, X2, Z, np.power(10,(np.arange(-20, 0.1, 3)).T))\n",
    "\n",
    "\n",
    "print('Visualizing Gaussian fit.')\n",
    "\n",
    "# Estimate mu and sigma2.\n",
    "mu, sigma2 = estimateGaussian(X)\n",
    "\n",
    "# Return the density of the multivariate normal at each data point (row) of X.\n",
    "p = multivariateGaussian(X, mu, sigma2)\n",
    "\n",
    "# Visualize the fit.\n",
    "visualizeFit(X, mu, sigma2)\n",
    "plt.xlabel('Latency (ms)')\n",
    "plt.ylabel('Throughput (mb/s)')\n",
    "plt.title('Figure 2: The Gaussian distribution contours \\\n",
    "of the distribution fit to the dataset.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting the Threshold, $\\varepsilon$\n",
    "\n",
    "Now that the Gaussian parameters have been estimated, it can be investigated which examples have a very high probability given this distribution and which examples have a very low probability. The low probability examples are more likely to be the anomalies in the dataset. One way to determine which examples are anomalies is to select a threshold based on a cross validation set. An algorithm will be implemented to select the threshold $\\varepsilon$ using the $F_1$ score on a cross validation set.\n",
    "\n",
    "The code in `selectThreshold` uses a cross validation set $\\left\\{\\left(x_{cv}^{\\left(1\\right)}, y_{cv}^{\\left(1\\right)}\\right),\\dots, \\left(x_{cv}^{\\left(m_{cv}\\right)}, y_{cv}^{\\left(m_{cv}\\right)} \\right) \\right\\}$, where the label $y = 1$ corresponds to an anomalous example, and $y = 0$ corresponds to a normal example. For each cross validation example, $p\\left(x_{cv}^{\\left(i\\right)}\\right)$ will be computed. The vector of all of these probabilities $p\\left(x_{cv}^{\\left(1\\right)}\\right), \\dots, p\\left(x_{cv}^{\\left(m_{cv}\\right)} \\right)$ is passed to `selectThreshold` in the vector `pval`. The corresponding labels $y_{cv}^{\\left(1\\right)},\\dots,y_{cv}^{\\left(m_{cv}\\right)}$ is passed to the same function in the vector `yval`.\n",
    "\n",
    "The function `selectThreshold` returns two values; the first is the selected threshold $\\varepsilon$. If an example $x$ has a low probability $p(x) < \\varepsilon$, then it is considered to be an anomaly. The function also returns the $F_1$ score, which tells how well it is doing on finding the ground truth anomalies given a certain threshold. For many different values of $\\varepsilon$, the resulting $F_1$ score will be computed by computing how many examples the current threshold classifies correctly and incorrectly.\n",
    "\n",
    "The $F_1$ score is computed using precision $(prec)$ and recall $(rec)$:\n",
    "\n",
    "$$ F_1 = \\frac{2\\cdot prec \\cdot rec}{prec + rec}, $$\n",
    "\n",
    "Precision and recall are computed by:\n",
    "\n",
    "$$ prec = \\frac{tp}{tp + fp} $$\n",
    "\n",
    "$$ rec = \\frac{tp}{tp + fn}, $$\n",
    "\n",
    "where\n",
    "* $tp$ is the number of true positives: the ground truth label says itâ€™s an anomaly and the algorithm correctly classified it as an anomaly.\n",
    "* $fp$ is the number of false positives: the ground truth label says itâ€™s not an anomaly, but the algorithm incorrectly classified it as an anomaly.\n",
    "* $fn$ is the number of false negatives: the ground truth label says itâ€™s an anomaly, but the algorithm incorrectly classified it as not being anomalous.\n",
    "\n",
    "In the code `selectThreshold`, a loop will try many different values of $\\varepsilon$ and select the best $\\varepsilon$ based on the $F_1$ score. The computation of the $F_1$ score can be implemented using a for-loop over all the cross validation examples $\\left(\\text{to compute the values }tp, fp, fn\\right)$. The\n",
    "value for $\\varepsilon$ should be about $8.99e-05$. Figure 3 plots the anomalies in a circle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to find the best threshold epsilon.\n",
    "def selectThreshold(yval, pval):\n",
    "    \"\"\"\n",
    "    Finds the best threshold to use for selecting outliers\n",
    "    based on the results from a validation set (pval) and\n",
    "    the ground truth (yval).\n",
    "    Args:\n",
    "        yval       : array(# of cv examples,)\n",
    "        pval       : array(# of cv examples,)\n",
    "    Returns:\n",
    "        bestEpsilon: float\n",
    "        bestF1     : float\n",
    "    \"\"\"\n",
    "    # Init values.\n",
    "    bestEpsilon = 0\n",
    "    bestF1 = 0\n",
    "    F1 = 0\n",
    "\n",
    "    stepsize = (max(pval) - min(pval)) / 1000\n",
    "    for epsilon in np.arange(min(pval), max(pval), stepsize):\n",
    "        # Use predictions to get a binary vector of\n",
    "        # 0's and 1's of the outlier predictions.\n",
    "        predictions = pval < epsilon\n",
    "        tp = sum(((yval == 1) & (predictions == 1)))\n",
    "        fp = sum((yval == 0) & (predictions == 1))\n",
    "        fn = sum((yval == 1) & (predictions == 0))\n",
    "        prec = tp / (tp + fp)\n",
    "        rec = tp / (tp + fn)\n",
    "        F1 = 2 * prec * rec / (prec + rec)\n",
    "        if F1 > bestF1:\n",
    "            bestF1 = F1\n",
    "            bestEpsilon = epsilon\n",
    "\n",
    "    return bestEpsilon, bestF1\n",
    "\n",
    "\n",
    "pval = multivariateGaussian(Xval, mu, sigma2)\n",
    "epsilon, F1 = selectThreshold(yval, pval)\n",
    "\n",
    "print('Best epsilon found using cross-validation: {}'.format(epsilon))\n",
    "print('Best F1 on Cross Validation Set:  {}'.format(F1))\n",
    "print('(A value epsilon of about 8.99e-05 is expected.)')\n",
    "\n",
    "# Find the outliers in the training set and plot them.\n",
    "outliers = p < epsilon\n",
    "\n",
    "# Draw a red circle around those outliers.\n",
    "plt.plot(X[outliers, 0], X[outliers, 1], 'ro', markersize=10, fillstyle='none')\n",
    "visualizeFit(X, mu, sigma2)\n",
    "plt.xlabel('Latency (ms)')\n",
    "plt.ylabel('Throughput (mb/s)')\n",
    "plt.title('Figure 3: The classified anomalies.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## High Dimensional Dataset\n",
    "\n",
    "The anomaly detection algorithm will be applied now on a more realistic and much harder dataset. In this dataset, each example is described by $11$ features, capturing many more properties of the compute servers.\n",
    "\n",
    "The code will estimate the Gaussian parameters $\\left(\\mu_i \\text{ and } \\sigma_i^2\\right)$, evaluate the probabilities for both the training data $X$ from which the Gaussian parameters were estimated, and do so for the the cross-validation set $Xval$. Finally, the `selectThreshold` will be used to find the best threshold $\\varepsilon$. A value epsilon of about $1.38e-18$, and $117$ anomalies shall be found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the second dataset.\n",
    "data2 = loadmat('ex8data2.mat')\n",
    "X = data2[\"X\"]\n",
    "Xval = data2[\"Xval\"]\n",
    "yval = data2[\"yval\"].flatten()\n",
    "\n",
    "# Apply the same steps to the larger dataset.\n",
    "mu, sigma2 = estimateGaussian(X)\n",
    "\n",
    "# Training set. \n",
    "p = multivariateGaussian(X, mu, sigma2)\n",
    "\n",
    "# Cross-validation set.\n",
    "pval = multivariateGaussian(Xval, mu, sigma2)\n",
    "\n",
    "# Find the best threshold.\n",
    "epsilon, F1 = selectThreshold(yval, pval)\n",
    "\n",
    "print('Best epsilon found using cross-validation: {}'.format(epsilon))\n",
    "print('Best F1 on Cross Validation Set: {}'.format(F1))\n",
    "print('# Outliers found: {}'.format(sum(p < epsilon)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommender Systems\n",
    "\n",
    "The collaborative filtering learning algorithm will be implemented and applied to a dataset of movie ratings. This dataset consists of ratings on a scale of 1 to 5. The dataset has $n_u = 943$ users, and $n_m = 1682$ movies.\n",
    "\n",
    "The code in the function `cofiCostFunc` computes the collaborative fitlering objective function and gradient. The function `minimize` will be used to learn the parameters for collaborative filtering.\n",
    "\n",
    "## Movie Ratings Dataset\n",
    "\n",
    "The variable $Y$ (a `num_movies x num_users` matrix) stores the ratings $y^{(i,j)}$ (from 1 to 5). The variable $R$ is a binary-valued indicator matrix, where $R(i, j) = 1$ if user $j$ gave a rating to movie $i$, and $R(i, j) = 0$ otherwise. The objective of collaborative filtering is to predict movie ratings for the movies that users have not yet rated, that is, the entries with $R(i, j) = 0$. This will allow to recommend the movies with the highest predicted ratings to the user.\n",
    "\n",
    "To understand the matrix $Y$, the average movie rating will be computed for the first movie (Toy Story) and output the average rating to the screen.\n",
    "\n",
    "The matrices, $X$ and $Theta$ will be also used:\n",
    "\n",
    "$$X = \\begin{bmatrix}\n",
    "    -\\left(x^{(1)}\\right)^T-\\\\\n",
    "    -\\left(x^{(2)}\\right)^T-\\\\\n",
    "    \\vdots\\\\\n",
    "    -\\left(x^{(n_m)}\\right)^T-\\end{bmatrix}, \\quad\n",
    "    Theta = \\begin{bmatrix}\n",
    "    -\\left(\\theta^{(1)}\\right)^T-\\\\\n",
    "    -\\left(\\theta^{(2)}\\right)^T-\\\\\n",
    "    \\vdots\\\\\n",
    "    -\\left(\\theta^{(n_u)}\\right)^T-\\end{bmatrix}.$$\n",
    "    \n",
    "The $i-th$ row of $X$ corresponds to the feature vector $x^{(i)}$ for the $i-th$ movie, and the $j-th$ row of $Theta$ corresponds to one parameter vector $\\theta^{(j)}$, for the $j-th$ user. Both $x^{(i)}$ and $\\theta^{(j)}$ are $n$-dimensional vectors. $n = 100$ will be used, and therefore, $x^{(i)} \\in \\mathbb{R}^{100}$ and $\\theta^{(j)} \\in \\mathbb{R}^{100}$. Correspondingly, $X$ is a $n_m \\times 100$ matrix and $Theta$ is a $n_u \\times 100$ matrix.\n",
    "\n",
    "## Collaborative Filtering Learning Algorithm\n",
    "\n",
    "First the cost function (without regularization) will be implemented.\n",
    "\n",
    "The collaborative filtering algorithm in the setting of movie recommendations considers a set of $n$-dimensional parameter vectors $x^{(1)}, \\dots, x^{(n_m)} \\text{ and } \\theta^{(1)}, \\dots, \\theta^{(n_u)}$, where the model predicts the rating for movie $i$ by user $j$ as $y^{(i,j)} = \\left(\\theta^{(j)}\\right)^T x^{(i)}$. Given a dataset that consists of a set of ratings produced by some users on some movies, the parameter vectors $x^{(1)}, \\dots, x^{(n_m)}, \\theta^{(1)}, \\dots, \\theta^{(n_u)}$ that produce the best fit (minimizes the squared error) will be learnt.\n",
    "\n",
    "Note that the parameters to the function `minimize` are $X$ and $Theta$.\n",
    "\n",
    "### Collaborative Filtering Cost Function\n",
    "\n",
    "The collaborative filtering cost function (without regularization) is given by\n",
    "\n",
    "$$ J\\left(x^{(1)}, \\dots, x^{(n_m)}, \\theta^{(1)}, \\dots, \\theta^{(n_u)} \\right) = \\frac{1}{2} \\sum_{(i, j):r(i, j)=1} \\left(\\left(\\theta^{(j)}\\right)^T x^{(i)} - y^{(i,j)} \\right)^2.$$\n",
    "\n",
    "The `cofiCostFunc` function will return this cost in the variable $J$. Note that the cost should be accumulated for user $j$ and movie $i$ only if $R(i,j) = 1$. An output cost of $22.22$ is expected.\n",
    "\n",
    "**Implementation Note:** A vectorized implementation to compute $J$ is strongly recommended, since it will later be called many times by the optimization package `minimize`. To come up with a vectorized implementation, the following tip is helpful: The $R$ matrix can be used to set selected entries to 0. For example, $R * M$ will do an element-wise multiplication between $M$ and $R$; since $R$ only has elements with values either 0 or 1, this has the effect of setting the elements of $M$ to 0 only when the corresponding value in $R$ is 0. Hence, `np.sum(error * R)` is the sum of all the elements of $M$ for which the corresponding element in $R$ equals 1.\n",
    "\n",
    "### Collaborative Filtering Gradient\n",
    "\n",
    "Next the gradient will be implemented in `cofiCostFunc`. The code will return the variables `X_grad` and `Theta_grad`. Note that `X_grad` is a matrix of the same size as $X$ and similarly, `Theta_grad` is a matrix of the same size as $Theta$. The gradients of the cost function is given by:\n",
    "\n",
    "$$\\frac{\\partial{J}}{\\partial{x_k^{(i)}}}=\\sum_{j:r(i,j)=1} \\left(\\left(\\theta^{(j)}\\right)^T x^{(i)} - y^{(i,j)} \\right)\\theta_k^{(j)}$$\n",
    "\n",
    "$$\\frac{\\partial{J}}{\\partial{\\theta_k^{(j)}}}=\\sum_{j:r(i,j)=1} \\left(\\left(\\theta^{(j)}\\right)^T x^{(i)} - y^{(i,j)} \\right)x_k^{(i)}$$\n",
    "\n",
    "Note that the function returns the gradient for both sets of variables by unrolling them into a single vector. A gradient check (`checkCostFunction`) will be applied to numerically check the implementation of the gradients. If the implementation is correct, then the analytical and numerical gradients match up closely.\n",
    "\n",
    "**Implementation Note:** A vectorized implementation is recommended. The gradient can be implemented with a for-loop over movies (for computing $\\frac{\\partial{J}}{\\partial{x_k^{(i)}}}$) and a for-loop over users (for computing $\\frac{\\partial{J}}{\\partial{\\theta_k^{(j)}}}$). To perform the vectorization, it must be found a way to compute all the derivatives associated with $x_1^{(i)}, x_2^{(i)},\\dots, x_n^{(i)}$ (i.e., the derivative terms associated with the feature vector $x^{(i)}$) at the same time. The derivatives for the feature vector of the $i-th$ movie can be defined as:\n",
    "\n",
    "$$\\left(X_{grad}\\left(i,:\\right)\\right)^T=\\begin{bmatrix}\n",
    "    \\frac{\\partial{J}}{\\partial{x_1^{(i)}}}\\\\\n",
    "    \\frac{\\partial{J}}{\\partial{x_2^{(i)}}}\\\\\n",
    "    \\vdots\\\\\n",
    "    \\frac{\\partial{J}}{\\partial{x_n^{(i)}}}\\end{bmatrix}=\n",
    "    \\sum_{j:r(i,j)=1} \\left(\\left(\\theta^{(j)}\\right)^T x^{(i)} - y^{(i,j)} \\right)\\theta^{(j)}$$\n",
    "    \n",
    "As previously noted, an element-wise multiplication with $R$ is the trick. A similar method can be used to vectorize the derivatives with respect to $\\theta^{(j)}$ as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Loading movie ratings dataset.')\n",
    "\n",
    "# Load data.\n",
    "data3 = loadmat('ex8_movies.mat')\n",
    "\n",
    "# Y is a 1682x943 matrix, containing ratings (1-5)\n",
    "# of 1682 movies on 943 users.\n",
    "Y = data3[\"Y\"]\n",
    "# R is a 1682x943 matrix, where R(i,j) = 1 if and only if\n",
    "# user j gave a rating to movie i.\n",
    "R = data3[\"R\"]\n",
    "\n",
    "# From the matrix, statistics like average rating can be computed.\n",
    "print('Average rating for movie 1 (Toy Story): {0:.2f}/5'.\\\n",
    "      format(np.mean(Y[0, R[0, :]==1])))\n",
    "\n",
    "# Visualize the ratings matrix by plotting it with imshow.\n",
    "plt.imshow(Y, aspect='auto') \n",
    "plt.ylabel('Movies')\n",
    "plt.xlabel('Users')\n",
    "plt.show()\n",
    "\n",
    "# Load pre-trained weights (X, Theta, num_users, num_movies, num_features).\n",
    "data4 = loadmat('ex8_movieParams.mat')\n",
    "X = data4[\"X\"]\n",
    "Theta = data4[\"Theta\"]\n",
    "\n",
    "# Reduce the data set size so that this runs faster.\n",
    "num_users = 4 \n",
    "num_movies = 5 \n",
    "num_features = 3\n",
    "X = X[:num_movies, :num_features]\n",
    "Theta = Theta[:num_users, :num_features]\n",
    "Y = Y[:num_movies, :num_users]\n",
    "R = R[:num_movies, :num_users]\n",
    "\n",
    "# Create a function to compute the cost J and grad.\n",
    "def cofiCostFunc(params, Y, R, num_users, num_movies, num_features, lambda_coef):\n",
    "    \"\"\"\n",
    "    Returns the cost and gradient for\n",
    "    the collaborative filtering problem.\n",
    "    Args:\n",
    "        params      : array(num_movies x num_features + num_users x num_features,)\n",
    "        Y           : array(num_movies, num_users)\n",
    "        R           : array(num_movies, num_users)\n",
    "        num_users   : int\n",
    "        num_movies  : int\n",
    "        num_features: int\n",
    "        lambda_coef : float\n",
    "    Returns:\n",
    "        J           : float\n",
    "        grad        : array(num_movies x num_features + num_users x num_features,)\n",
    "    \"\"\"\n",
    "    # Unfold params back into the parameters X and Theta.\n",
    "    X = np.reshape(params[:num_movies * num_features], (num_movies, num_features))\n",
    "    Theta = np.reshape(params[num_movies * num_features:], (num_users, num_features))\n",
    "    \n",
    "    # Init values.\n",
    "    J = 0\n",
    "    X_grad = np.zeros(X.shape)\n",
    "    Theta_grad = np.zeros(Theta.shape)\n",
    "    \n",
    "    # Compute squared error.\n",
    "    error = np.square(np.dot(X, Theta.T) - Y)\n",
    "    \n",
    "    # Compute regularization term.\n",
    "    reg_term = (lambda_coef / 2) * (np.sum(np.square(Theta)) + np.sum(np.square(X)))\n",
    "\n",
    "    # Compute cost function but sum only if R(i,j)=1; vectorized solution.\n",
    "    J = (1 / 2) * np.sum(error * R) + reg_term\n",
    "    \n",
    "    # Compute the gradients.\n",
    "    X_grad = np.dot((np.dot(X, Theta.T) - Y) * R, Theta) + lambda_coef * X\n",
    "    Theta_grad = np.dot(((np.dot(X, Theta.T) - Y) * R).T, X) + lambda_coef * Theta\n",
    "    \n",
    "    grad = np.concatenate((X_grad.reshape(X_grad.size),\n",
    "                           Theta_grad.reshape(Theta_grad.size)))\n",
    "    \n",
    "    return J, grad\n",
    "\n",
    "\n",
    "# Create a list of my X and Theta.\n",
    "lst_params = [X, Theta]\n",
    "\n",
    "# Unroll parameters and then merge/concatenate.\n",
    "unrolled_params = [lst_params[i].ravel() for i,_ in enumerate(lst_params)]\n",
    "params = np.concatenate(unrolled_params)\n",
    "\n",
    "# Evaluate cost function.\n",
    "J, _ = cofiCostFunc(params, Y, R, num_users, num_movies, num_features, 0)\n",
    "           \n",
    "print('Cost at loaded parameters: {:0.2f}'.format(J))\n",
    "print('(this value should be about 22.22)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularized Cost Function\n",
    "\n",
    "The cost function for collaborative filtering with regularization is given by:\n",
    "\n",
    "$$ J\\left(x^{(1)}, \\dots, x^{(n_m)}, \\theta^{(1)}, \\dots, \\theta^{(n_u)} \\right) = \\frac{1}{2} \\sum_{(i, j):r(i, j)=1} \\left(\\left(\\theta^{(j)}\\right)^T x^{(i)} - y^{(i,j)} \\right)^2 + \\left(\\frac{\\lambda}{2}\\sum_{j=1}^{n_u}\\sum_{k=1}^n\\left(\\theta_k^{(j)}\\right)^2\\right) + \\left(\\frac{\\lambda}{2}\\sum_{i=1}^{n_m}\\sum_{k=1}^n\\left(x_k^{(i)}\\right)^2\\right).$$\n",
    "\n",
    "### Regularized Gradient\n",
    "\n",
    "The gradients for the regularized cost function is given by:\n",
    "\n",
    "$$\\frac{\\partial{J}}{\\partial{x_k^{(i)}}}=\\sum_{j:r(i,j)=1} \\left(\\left(\\theta^{(j)}\\right)^T x^{(i)} - y^{(i,j)} \\right)\\theta_k^{(j)} + \\lambda x_k^{(i)}$$\n",
    "\n",
    "$$\\frac{\\partial{J}}{\\partial{\\theta_k^{(j)}}}=\\sum_{j:r(i,j)=1} \\left(\\left(\\theta^{(j)}\\right)^T x^{(i)} - y^{(i,j)} \\right)x_k^{(i)} + \\lambda \\theta_k^{(j)}.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to compute numerical gradient.\n",
    "def computeNumericalGradient(J, theta):\n",
    "    \"\"\"\n",
    "    Computes the numerical gradient of the function J\n",
    "    around theta using \"finite differences\" and gives\n",
    "    a numerical estimate of the gradient.\n",
    "    Notes: The following code implements numerical\n",
    "           gradient checking, and returns the numerical\n",
    "           gradient. It sets numgrad(i) to (a numerical \n",
    "           approximation of) the partial derivative of J\n",
    "           with respect to the i-th input argument,\n",
    "           evaluated at theta. (i.e., numgrad(i) should\n",
    "           be the (approximately) the partial derivative\n",
    "           of J with respect to theta(i).)\n",
    "    Args:\n",
    "        J      : function\n",
    "        theta  : array(num_movies x num_features + num_users x num_features,)\n",
    "    Returns:\n",
    "        numgrad: array(num_movies x num_features + num_users x num_features,)\n",
    "    \"\"\"\n",
    "    # Initialize parameters.\n",
    "    numgrad = np.zeros(theta.shape)\n",
    "    perturb = np.zeros(theta.shape)\n",
    "    e = 1e-4\n",
    "\n",
    "    for p in range(theta.size):\n",
    "        # Set the perturbation vector.\n",
    "        perturb.reshape(perturb.size)[p] = e\n",
    "        loss1, _ = J(theta - perturb)\n",
    "        loss2, _ = J(theta + perturb)\n",
    "        # Compute the Numerical Gradient.\n",
    "        numgrad.reshape(numgrad.size)[p] = (loss2 - loss1) / (2 * e)\n",
    "        perturb.reshape(perturb.size)[p] = 0\n",
    "    \n",
    "    return numgrad\n",
    "\n",
    "# Create a function to check the cost function and gradients.\n",
    "def checkCostFunction(lambda_coef):\n",
    "    \"\"\"\n",
    "    Creates a collaborative filering problem \n",
    "    to check the cost function and gradients.\n",
    "    It will output the analytical gradients\n",
    "    and the numerical gradients computed using\n",
    "    computeNumericalGradient. These two gradient \n",
    "    computations should result in very similar values.\n",
    "    Args:\n",
    "        lambda_coef : float\n",
    "    \"\"\"\n",
    "    # Create small problem.\n",
    "    X_t = np.random.rand(4, 3)\n",
    "    Theta_t = np.random.rand(5, 3)\n",
    "    \n",
    "    # Zap out most entries.\n",
    "    Y = np.dot(X_t, Theta_t.T)\n",
    "    Y[np.random.rand(Y.shape[0], Y.shape[1]) > 0.5] = 0\n",
    "    R = np.zeros(Y.shape)\n",
    "    R[Y != 0] = 1\n",
    "\n",
    "    # Run Gradient Checking.\n",
    "    X = np.random.randn(X_t.shape[0], X_t.shape[1])\n",
    "    Theta = np.random.randn(Theta_t.shape[0], Theta_t.shape[1])\n",
    "    num_users = Y.shape[1]\n",
    "    num_movies = Y.shape[0]\n",
    "    num_features = Theta_t.shape[1]\n",
    "    \n",
    "    # Create short hand for cost function.\n",
    "    def costFunc(p):\n",
    "        return cofiCostFunc(p, Y, R, num_users, num_movies,\n",
    "                            num_features, lambda_coef)\n",
    "\n",
    "    params = np.concatenate((X.reshape(X.size), Theta.reshape(Theta.size)))\n",
    "    numgrad = computeNumericalGradient(costFunc, params)\n",
    "    J, grad = cofiCostFunc(params, Y, R, num_users, num_movies,\n",
    "                           num_features, lambda_coef)\n",
    "    \n",
    "    # Visually examine the two gradient computations.\n",
    "    for numerical, analytical in zip(numgrad, grad):\n",
    "        print('Numerical Gradient: {0:10f}, Analytical Gradient {1:10f}'.\\\n",
    "              format(numerical, analytical))\n",
    "    print('\\nThe above two columns should be very similar.\\n')\n",
    "        \n",
    "    # Evaluate the norm of the difference between two solutions.\n",
    "    diff = np.linalg.norm(numgrad-grad) / np.linalg.norm(numgrad+grad)\n",
    "\n",
    "    print('If the backpropagation implementation is correct, then \\n' \\\n",
    "             'the relative difference will be small (less than 1e-9). \\n' \\\n",
    "             '\\nRelative Difference: {:.10E}'.format(diff))\n",
    "\n",
    "    \n",
    "print('Checking Gradients (without regularization)...\\n')\n",
    "# Check gradients by running checkCostFunction.\n",
    "checkCostFunction(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate cost function.\n",
    "J, _ = cofiCostFunc(params, Y, R, num_users, num_movies, num_features, 1.5)\n",
    "           \n",
    "print('Cost at loaded parameters (lambda_coef = 1.5): {:0.2f}'\\\n",
    "         '\\n(this value should be 31.34)\\n'.format(J))\n",
    "\n",
    "print('Checking Gradients (with regularization)... \\n')\n",
    "\n",
    "# Check gradients by running checkCostFunction.\n",
    "checkCostFunction(1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Movie Recommendations\n",
    "\n",
    "Now the algorithm can be trained to make movie recommendations. First, an example of movie preferences will be entered, so that later when the algorithm runs, the movie recommendations can be computed based on the preferences. The list of all movies and their number in the dataset can be found listed in the file `movie_idx.txt`.\n",
    "\n",
    "### Recommendations\n",
    "\n",
    "After the additional ratings have been added to the dataset, the collaborative filtering model will be trained. This will learn the parameters $X$ and $Theta$. To predict the rating of movie $i$ for user $j$, the $\\left(\\theta^{(j)}\\right)^T x^{(i)}$ need to be computed. Then, the ratings are computed for all the movies and users and the movies that are recommended are displayed, according to ratings that were entered earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to load movies.\n",
    "def loadMovieList():\n",
    "    \"\"\"\n",
    "    Reads the fixed movie list in movie_idx.txt\n",
    "    and returns a cell array of the words in movieList.\n",
    "    Returns:\n",
    "        movieList: list\n",
    "    \"\"\"\n",
    "    # Read the fixed movieulary list.\n",
    "    with open('movie_ids.txt', encoding = \"ISO-8859-1\") as f:\n",
    "        movieList = []\n",
    "        for line in f:\n",
    "            movieName = line.split()[1:]\n",
    "            movieList.append(\" \".join(movieName))\n",
    "\n",
    "    return movieList\n",
    "\n",
    "movieList = loadMovieList()\n",
    "\n",
    "# Initialize ratings.\n",
    "my_ratings = np.zeros((1682, 1))\n",
    "\n",
    "# Check the file movie_idx.txt for id of each movie in the dataset.\n",
    "# For example, Toy Story (1995) has ID 0, so to rate it \"4\", set:\n",
    "my_ratings[0] = 4\n",
    "\n",
    "# Or suppose did not enjoy The Mask (1994), so set:\n",
    "my_ratings[71] = 1\n",
    "\n",
    "# Select a few movies and rate them:\n",
    "my_ratings[8] = 3\n",
    "my_ratings[12]= 3\n",
    "my_ratings[32]= 2\n",
    "my_ratings[44] = 5\n",
    "my_ratings[60] = 5\n",
    "my_ratings[63] = 4\n",
    "my_ratings[67] = 3\n",
    "my_ratings[85] = 5\n",
    "my_ratings[117] = 1\n",
    "my_ratings[153] = 4\n",
    "my_ratings[155] = 5\n",
    "my_ratings[164] = 5\n",
    "my_ratings[174] = 4\n",
    "my_ratings[178] = 5\n",
    "my_ratings[193] = 4\n",
    "my_ratings[354] = 2\n",
    "my_ratings[442] = 4\n",
    "my_ratings[478] = 5\n",
    "my_ratings[514] = 5\n",
    "my_ratings[606] = 5\n",
    "my_ratings[633] = 5\n",
    "my_ratings[639] = 5\n",
    "my_ratings[649] = 5\n",
    "my_ratings[954] = 5\n",
    "my_ratings[1422] = 3\n",
    "\n",
    "print('User ratings:\\n')\n",
    "for i, rating in enumerate(my_ratings):\n",
    "    if rating > 0: \n",
    "        print('Rated {} for {}'.format(rating[0], movieList[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Movie Ratings\n",
    "\n",
    "Now, the collaborative filtering model will be trained on a movie rating dataset of 1682 movies and 943 users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "print('Training collaborative filtering...')\n",
    "\n",
    "# Load data.\n",
    "Y = data3[\"Y\"] # array(1682, 943)\n",
    "R = data3[\"R\"] # array(1682, 943)\n",
    "\n",
    "# Add my ratings to the data matrix.\n",
    "Y = np.column_stack((my_ratings, Y)) # array(1682, 944)\n",
    "R = np.column_stack(((my_ratings != 0), R)) # array(1682, 944)\n",
    "\n",
    "# Create a function to normalize ratings.\n",
    "def normalizeRatings(Y, R):\n",
    "    \"\"\"\n",
    "    Preprocesses data by subtracting mean rating for every\n",
    "    movie (every row). Normalizes Y so that each movie has\n",
    "    a rating of 0 on average, and returns the mean rating in Ymean.\n",
    "    Args:\n",
    "        Y    : array(num_movies, num_users)\n",
    "        R    : array(num_movies, num_users)\n",
    "    Returns:\n",
    "        Ynorm: array(num_movies, num_users)\n",
    "        Ymean: array(num_movies, 1)\n",
    "    \"\"\"\n",
    "    m, n = Y.shape\n",
    "    Ymean = np.zeros((m, 1))\n",
    "    Ynorm = np.zeros(Y.shape)\n",
    "    for i in range(m):\n",
    "        idx = R[i, :] == 1\n",
    "        # Compute the mean only of the rated movies.\n",
    "        Ymean[i] = np.mean(Y[i, idx])\n",
    "        Ynorm[i, idx] = Y[i, idx] - Ymean[i]\n",
    "\n",
    "    return Ynorm, Ymean\n",
    "\n",
    "# Normalize ratings.\n",
    "[Ynorm, Ymean] = normalizeRatings(Y, R)\n",
    "\n",
    "# Get useful values.\n",
    "num_users = Y.shape[1]\n",
    "num_movies = Y.shape[0]\n",
    "num_features = 10\n",
    "\n",
    "# Set initial parameters (Theta, X).\n",
    "X = np.random.randn(num_movies, num_features)\n",
    "Theta = np.random.randn(num_users, num_features)\n",
    "\n",
    "initial_parameters = np.concatenate((X.reshape(X.size),\n",
    "                                     Theta.reshape(Theta.size)))\n",
    "\n",
    "# Set options.\n",
    "options = {'maxiter': 100, 'disp': True}\n",
    "\n",
    "# Set regularization.\n",
    "lambda_coef = 10\n",
    "\n",
    "# Create short hand for cost function.\n",
    "def costFunc(initial_parameters):\n",
    "    return cofiCostFunc(initial_parameters, Y, R, num_users,\n",
    "                        num_movies, num_features, lambda_coef)\n",
    "\n",
    "# Optimize.\n",
    "results = minimize(costFunc, x0=initial_parameters,\n",
    "                   options=options, method='CG', jac=True)\n",
    "theta = results.x\n",
    "\n",
    "# Unfold results back into the parameters X and Theta.\n",
    "X = np.reshape(theta[:num_movies * num_features], (num_movies, num_features))\n",
    "Theta = np.reshape(theta[num_movies * num_features:], (num_users, num_features))\n",
    "\n",
    "print('\\nRecommender system learning completed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recommendation for me\n",
    "\n",
    "After training the model, recommendations can be generated by computing the predictions matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = np.dot(X, Theta.T)\n",
    "# Get first column (my predictions) and add Ymean.\n",
    "my_predictions = p[:, 0] + Ymean.flatten()\n",
    "\n",
    "movieList = loadMovieList()\n",
    "\n",
    "# Reverse sort by index.\n",
    "ix = my_predictions.argsort()[::-1]\n",
    "\n",
    "print('Top 10 recommendations for me:\\n')\n",
    "for i in range(10):\n",
    "    j = ix[i]\n",
    "    print('Predicting rating {:0.2f} for movie {}'.\\\n",
    "          format(my_predictions[j], movieList[j]))\n",
    "\n",
    "print('\\n\\nOriginal ratings provided:\\n')\n",
    "for i in range(len(my_ratings)):\n",
    "    if my_ratings[i] > 0:\n",
    "        print('Rated {} for {}'.\\\n",
    "              format(int(my_ratings[i]), movieList[i]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
