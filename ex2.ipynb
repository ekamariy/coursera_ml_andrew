{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Logistic Regression\n",
    "\n",
    "A logistic regression model will be implemented to predict whether a student gets admitted into a university. Historical data from previous applicants will be used as a training set. Each training example includes the applicant's scores on two exams and the admissions decision. A classification model that estimates an applicant's\n",
    "probability of admission based on the scores from those two exams will be built.\n",
    "\n",
    "Read the data into a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "\n",
    "data1 = pd.read_csv('ex2data1.txt', header=None, names=['Score1', 'Score2', 'Admitted'])\n",
    "data1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the Data\n",
    "\n",
    "Plot the data creating a figure, where the axes are the two exam scores, and the positive and negative examples are shown with different markers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.xlabel('Exam 1 score')\n",
    "plt.ylabel('Exam 2 score')\n",
    "plt.title('Scatter plot of training data')\n",
    "plt.plot(data1['Score1'][data1['Admitted']==1],\n",
    "         data1['Score2'][data1['Admitted']==1], 'k+',\n",
    "         label='Admitted')\n",
    "plt.plot(data1['Score1'][data1['Admitted']==0],\n",
    "         data1['Score2'][data1['Admitted']==0], 'yo',\n",
    "         label='Not Admitted')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "### Sigmoid Function\n",
    "\n",
    "The logistic regression hypothesis is defined as:\n",
    "\n",
    "$h_{\\theta}(x) = g(\\theta^{T} x)$\n",
    "\n",
    "where function g is the sigmoid function. The sigmoid function is defined as:\n",
    "\n",
    "$g(z) = \\frac{1}{1+e^{-z}}$\n",
    "\n",
    "The first step is to implement the sigmoid function. For large positive values of $x$, the sigmoid should be close to 1, while for large negative values, the sigmoid should be close to 0. Evaluating sigmoid(0) should give exactly 0.5. The code should also work with vectors and matrices. For a matrix, the function should perform the sigmoid function on every element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the sigmoid function.\n",
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    The sigmoid function.\n",
    "    Args:\n",
    "        z: float, vector, matrix\n",
    "    Returns:\n",
    "        sigmoid: float, vector, matrix\n",
    "    \"\"\"\n",
    "    sigmoid = 1 / (1 + np.exp(-z))\n",
    "    return sigmoid\n",
    "\n",
    "# Visualize the sigmoid function\n",
    "x = np.arange(-10, 10, 0.1)\n",
    "plt.axhline(y=0.5, c='black', ls=':')\n",
    "plt.axvline(x=0, c='black', ls=':')\n",
    "plt.annotate('(0, 0.5)', xy=(0, 0.5), xytext=(-3, 0.55))\n",
    "plt.plot(x, sigmoid(x))\n",
    "plt.title('Sigmoid Function')\n",
    "plt.show()\n",
    "\n",
    "print('Evaluate sigmoid(0) =', sigmoid(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost Function and Gradient\n",
    "\n",
    "Implement the cost function and gradient for logistic regression. The cost function in logistic regression is:\n",
    "\n",
    "$J(\\theta)=\\frac{1}{m} \\sum_{i=1}^m[-y^{(i)} log(h_\\theta (x^{(i)})-(1-y^{(i)}) log(1-h_\\theta (x^{(i)}))]$\n",
    "\n",
    "and the gradient of the cost is a vector of the same length as $\\theta$ where the $j^{th}$ element (for j = 0, 1,..., n) is defined as follows:\n",
    "\n",
    "$\\frac{\\partial J(\\theta)}{\\partial \\theta_j}=\\frac{1}{m} \\sum_{i=1}^m(h_\\theta(x^{(i)})-y^{(i)})x_j^{(i)}$\n",
    "\n",
    "While this gradient looks identical to the linear regression gradient, the formula is actually different because linear and logistic regression have different definitions of $h_{\\theta}(x)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of features.\n",
    "n = len(data1.columns)-1 # subtract 1 for the target column\n",
    "\n",
    "# Create a function to pepare the data.\n",
    "def prepareData(data, n):\n",
    "    \"\"\"\n",
    "    Add 1s column, convert to arrays,\n",
    "    initialize theta.\n",
    "    Args:\n",
    "        data: read the data file\n",
    "        n: int\n",
    "    Return:\n",
    "        x: a (m, n+1) array\n",
    "        y: a (m, 1) array\n",
    "        theta: a (n+1, 1) array\n",
    "    \"\"\"\n",
    "    # Add a column with 1s in the data set.\n",
    "    data.insert(0, 'Ones', 1)\n",
    "\n",
    "    # Define x and y, separating the data set.\n",
    "    x = data.iloc[:, 0:n+1]\n",
    "    y = data.iloc[:, n+1:n+2]\n",
    "\n",
    "    # Convert to arrays, so this function is\n",
    "    # compatible with scipy.optimize.fmin later.\n",
    "    # Initialize parameters theta to 0s.\n",
    "    # Theta is a (n+1, 1) array,\n",
    "    # where n is the number of features.\n",
    "    x = np.array(x.values)\n",
    "    y = np.array(y.values)\n",
    "    theta = np.zeros((n+1, 1))\n",
    "    return x, y, theta\n",
    "\n",
    "x, y, theta = prepareData(data1, n)\n",
    "\n",
    "print('Checking the dimensions of the matrices x, y, theta...')\n",
    "print(x.shape, y.shape, theta.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to compute the cost.\n",
    "# NOTE!!! The parameters must be in the right order!!!.\n",
    "# The computeCost(theta, x, y) is the correct order,\n",
    "# assuming that x and y in computeCost are meant\n",
    "# to match with the args=(x, y) later in fmin function.\n",
    "def computeCost(theta, x, y):\n",
    "    \"\"\"\n",
    "    Compute the cost function.\n",
    "    Args:\n",
    "        theta: array shape(n+1, 1) \n",
    "        x: array shape(m, n+1) \n",
    "        y: array shape(m, 1)\n",
    "    Returns:\n",
    "        cost: float\n",
    "    \"\"\"\n",
    "    m = len(x)\n",
    "    # Do matrix multiplication with numpy.dot\n",
    "    h_theta = sigmoid(np.dot(x, theta))\n",
    "    term1 = np.dot(-y.T, np.log(h_theta))\n",
    "    term2 = np.dot((1 - y).T, np.log(1 - h_theta))\n",
    "    cost = np.sum(term1 - term2) / m\n",
    "    return cost\n",
    "\n",
    "print('The cost with initial Î¸ equals to zeros, is:')\n",
    "print(computeCost(theta, x, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Parameters Using the `scipy.optimize.fmin` Algorithm\n",
    "\n",
    "In the previous exercise 1, the optimal parameters of a linear regression model was computed by implementing gradient descent. This time, instead of taking gradient descent steps, a Python function called `fmin` from `scipy` will be used. This is the simplest way to minimize a function (unconstrained and constrained). For logistic regression, the objective is to optimize the cost function $J(\\theta)$ with parameters $\\theta$. Concretely, `fmin` will be applied to find the best parameters $\\theta$ for the logistic regression cost function, given a fixed dataset (of $x$ and $y$ values). The following inputs will be passed to `fmin`:\n",
    "\n",
    "* The initial values of the parameters to be optimized.\n",
    "* A function that, when given the training set and a particular $\\theta$, computes the logistic regression cost and gradient with respect to $\\theta$ for the dataset ($x$, $y$ )."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import fmin\n",
    "\n",
    "def minimizeCost(computeCost, theta, x, y):\n",
    "    \"\"\"\n",
    "    Minimize the cost function.\n",
    "    Args:\n",
    "        computeCost: The function to be minimized\n",
    "        theta: initial guess, array shape(n+1, 1)\n",
    "        x: array shape(m, n+1)\n",
    "        y: array shape(m, 1)\n",
    "    Returns:\n",
    "        theta: array shape(n+1, 1), the parameters that minimize the cost\n",
    "    \"\"\"\n",
    "    min_cost = fmin(func=computeCost, x0=theta, args=(x, y))\n",
    "    return min_cost\n",
    "\n",
    "theta = minimizeCost(computeCost, theta, x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the optimal theta parameters and the cost.\n",
    "print(\"Optimal theta parameters:\")\n",
    "print(theta)\n",
    "print(\"\\nCost of optimal theta:\")\n",
    "print(computeCost(theta, x, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the Decision Boundary\n",
    "\n",
    "Plot the decision boundary on the training data, using the optimal $\\theta$ values. The boundary is exactly at sigmoid(0) = 0.5:\n",
    "\n",
    "$\\frac{1}{1+e^{-(\\theta_0+\\theta_1x_1+\\theta_2x_2)}}=0.5$\n",
    "\n",
    "or\n",
    "\n",
    "$\\theta_0+\\theta_1x_1+\\theta_2x_2=0$\n",
    "\n",
    "and with the following conversion\n",
    "\n",
    "$x_2=-(\\frac{\\theta_0+\\theta_1x_1}{\\theta_2})$\n",
    "\n",
    "the boundary decision is graphically plotted.\n",
    "\n",
    "### Evaluating logistic regression\n",
    "\n",
    "Use the model to predict whether a particular student will be admitted. For a student with an Exam 1 score of 45 and an Exam 2 score of 85, it is expected an admission probability of 0.776. Another way to evaluate the quality of the parameters is to see how well the learned model predicts on the training set. Create a predict function that will produce â1â or â0â predictions given a dataset and a learned parameter vector Î¸. Finally, report the training accuracy of the classifier by computing the percentage of examples it got correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only 2 points are required to define a line, e.g. min and max.\n",
    "plot_x = np.array([np.min(x[:, 1]) - 2, np.max(x[:, 1] + 2)])\n",
    "plot_y = -(theta[0] + theta[1] * plot_x) / theta[2]\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.xlabel('Exam 1 score')\n",
    "plt.ylabel('Exam 2 score')\n",
    "plt.title('Scatter plot of training data')\n",
    "plt.plot(data1['Score1'][data1['Admitted']==1],\n",
    "         data1['Score2'][data1['Admitted']==1], 'k+',\n",
    "         label='Admitted')\n",
    "plt.plot(data1['Score1'][data1['Admitted']==0],\n",
    "         data1['Score2'][data1['Admitted']==0], 'yo',\n",
    "         label='Not Admitted')\n",
    "plt.plot(plot_x, plot_y, 'b:', label='Decision Boundary')\n",
    "plt.plot(45, 85, 'rx', ms=10)\n",
    "plt.annotate('(45, 85)', xy=(45, 85), xytext=(47, 84))\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "probability = sigmoid(np.dot(np.array([1, 45, 85]), theta))\n",
    "print('Admission probability for scores (45, 85)=', probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a prediction function.\n",
    "def predictAdmission(x, theta):\n",
    "    \"\"\"\n",
    "    Predict whether a student will be admitted.\n",
    "    Args:\n",
    "        x: array shape(m, n+1)\n",
    "        theta: ndarray, the optimal parameters of the cost function\n",
    "    Returns:\n",
    "        predicted: array shape(m,) of booleans\n",
    "    \"\"\"\n",
    "    probability = np.array(sigmoid(np.dot(x, theta)))\n",
    "    predicted = probability >= 0.5\n",
    "    return predicted\n",
    "\n",
    "predicted = predictAdmission(x, theta)\n",
    "correct = np.sum(predicted.astype(int) == y.reshape(100))\n",
    "total = len(predicted)\n",
    "print('Accuracy score: {}%'.format(100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Regularized Logistic Regression\n",
    "\n",
    "A regularized logistic regression will be implemented to predict whether microchips from a fabrication plant passes quality assurance (QA). During QA, each microchip goes through various tests to ensure it is functioning correctly. A dataset of test results on past microchips will be used to build a logistic regression model. There are test results for some microchips on two different tests. From these two tests, it will be determined whether the microchips should be accepted or rejected.\n",
    "\n",
    "Read the data into a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = pd.read_csv('ex2data2.txt', header=None, names=['Test1', 'Test2', 'Accepted'])\n",
    "data2.sample(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the Data\n",
    "\n",
    "Plot the data creating a figure, where the axes are the two test scores, and the positive ( $y=1$, accepted) and negative ( $y=0$, rejected) examples are shown with different markers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.xlabel('Microchip Test 1')\n",
    "plt.ylabel('Microchip Test 2')\n",
    "plt.title('Plot of training data')\n",
    "plt.plot(data2['Test1'][data2['Accepted']==1],\n",
    "         data2['Test2'][data2['Accepted']==1], 'k+',\n",
    "         label='y = 1')\n",
    "plt.plot(data2['Test1'][data2['Accepted']==0],\n",
    "         data2['Test2'][data2['Accepted']==0], 'yo',\n",
    "         label='y = 0')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scatter plot shows that the dataset cannot be separated into positive and negative examples by a straight-line through the plot. Therefore, a straightforward application of logistic regression will not perform well on this dataset since logistic regression will only be able to find a linear decision boundary.\n",
    "\n",
    "## Feature mapping\n",
    "\n",
    "One way to fit the data better is to create more features from each data point, for example mapping the features into all polynomial terms of $x_1$ and $x_2$ up to the sixth power. As a result of this mapping, the matrix of two features (the scores on two QA tests) will be transformed into a 28-dimensional matrix. A logistic regression classifier trained on this higher-dimension feature matrix will have a more complex decision boundary and will appear nonlinear when drawn in the 2-dimensional plot. Note that while the feature mapping allows to build a more expressive classifier, it is also more susceptible to overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def mapFeature(data, features, degree):\n",
    "    \"\"\"\n",
    "    Feature mapping function to polynomial features.\n",
    "    Maps the features to quadratic features.\n",
    "    Returns a new df with more features, comprising of\n",
    "    x1, x2, x1^2, x2^2, x1*x2, x1*x2^2, etc...\n",
    "    Args:\n",
    "        data: a pandas df with index, features and target cols\n",
    "        features: int, the number of initial features\n",
    "        degree: int, the polynomial degree\n",
    "    Returns:\n",
    "        df: a pandas df with the new features\n",
    "    \"\"\"\n",
    "    # Create a copy of the data df.\n",
    "    df = copy.deepcopy(data)\n",
    "    degree = degree\n",
    "    # Insert a first column with ones.\n",
    "    df.insert(0, 'Ones', 1)\n",
    "    x = []\n",
    "    for n in range(1, features + 1):\n",
    "        x.append(df.iloc[:, n])\n",
    "    for i in range(1, degree + 1):\n",
    "        for j in range(0, i + 1):\n",
    "            df['x' + str(i) + str(j)] = np.power(x[0], i-j) * np.power(x[1], j)\n",
    "    \n",
    "    # Drop unnecessary columns.\n",
    "    cols = [0, 1, 2]\n",
    "    df.drop(df.columns[cols], axis=1, inplace=True)\n",
    "    # Move target column to the end.\n",
    "    # Make a list of all of the columns.\n",
    "    cols = df.columns.tolist()\n",
    "    # Reorder columns.\n",
    "    cols = cols[1:] + [cols[0]]\n",
    "    # Commit the reordering.\n",
    "    df = df[cols]\n",
    "    return df\n",
    "\n",
    "transformed_data = mapFeature(data2, features=2, degree=6)\n",
    "transformed_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost function and gradient\n",
    "\n",
    "Implement code to compute the cost function and gradient for regularized logistic regression. The regularized cost function in logistic regression is\n",
    "\n",
    "$J(\\theta)=\\frac{1}{m} \\sum_{i=1}^m[-y^{(i)} log(h_\\theta (x^{(i)})-(1-y^{(i)}) log(1-h_\\theta (x^{(i)}))]+\\frac{\\lambda}{2m} \\sum_{j=1}^n\\theta_j^2$\n",
    "\n",
    "Note that the parameter $\\theta_0$ should not be regularized. The gradient of the cost function is a vector where the $j^{th}$ element is defined as follows:\n",
    "\n",
    "$\\frac{\\partial J(\\theta)}{\\partial \\theta_0}=\\frac{1}{m} \\sum_{i=1}^m(h_\\theta(x^{(i)})-y^{(i)})x_j^{(i)}$ for $j=0$\n",
    "\n",
    "and\n",
    "\n",
    "$\\frac{\\partial J(\\theta)}{\\partial \\theta_j}=\\frac{1}{m} \\sum_{i=1}^m(h_\\theta(x^{(i)})-y^{(i)})x_j^{(i)}+\\frac{\\lambda}{m}\\theta_j$ for $j\\geq1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of features.\n",
    "n = len(transformed_data.columns)-1 # subtract the target column\n",
    "\n",
    "x, y, theta = prepareData(transformed_data, n)\n",
    "\n",
    "print('Checking the dimensions of the matrices x, y, theta...')\n",
    "print(x.shape, y.shape, theta.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the computeCost function to\n",
    "# include the regularization term.\n",
    "def computeCost(theta, x, y, lambda_coef):\n",
    "    \"\"\"\n",
    "    Compute the cost function.\n",
    "    Args:\n",
    "        theta: array shape(n+1, 1) \n",
    "        x: array shape(m, n+1) \n",
    "        y: array shape(m, 1)\n",
    "        lambda_coef: int\n",
    "    Returns:\n",
    "        cost: float\n",
    "    \"\"\"\n",
    "    m = len(x)\n",
    "    # Do matrix multiplication with numpy.dot\n",
    "    h_theta = sigmoid(np.dot(x, theta))\n",
    "    term1 = np.dot(-y.T, np.log(h_theta))\n",
    "    term2 = np.dot((1 - y).T, np.log(1 - h_theta))\n",
    "    # Exclude theta_0!!!\n",
    "    reg_term = (lambda_coef / (2 * m)) * np.sum(np.square(theta[1:]))\n",
    "    cost = (np.sum(term1 - term2) / m) + reg_term\n",
    "    return cost\n",
    "\n",
    "print('The cost with initial Î¸ equals to zeros, is:')\n",
    "print(computeCost(theta, x, y, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Parameters\n",
    "\n",
    "Similar to the previous parts, an optimize function will be used to learn the optimal parameters $\\theta$. The `fmin` was tested and it was found that cannot converge. The alternative `minimize` function will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "res = minimize(fun=computeCost, x0=theta, args=(x, y, 100))\n",
    "print(\"Optimal theta parameters for lambda = 100:\")\n",
    "theta_opt = res.x\n",
    "theta_opt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the Decision Boundary\n",
    "\n",
    "Plot the decision boundary on the training data, using the optimal $\\theta$ values. Compute the classifierâs predictions on an evenly spaced grid and then draw a contour plot of where the predictions change from $y=0$ to $y=1$.\n",
    "\n",
    "Notice the changes in the decision boundary as $\\lambda$ varies. With a small $\\lambda$, the classifier gets almost every training example correct, but draws a very complicated boundary, thus overfitting the data. This is not a good decision boundary: for example, it predicts that a point at $x = (â0.25, 1.5)$ is accepted $(y = 1)$, which seems to be an incorrect decision given the training set.\n",
    "\n",
    "With a larger $\\lambda$, the plot shows a simpler decision boundary which still separates the positives and negatives fairly well. However, if $\\lambda$ is set too high, the resulting fit is not good and the decision boundary will not follow the data so well, thus underfitting the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create different cases of Î» to plot the decision boundaries.\n",
    "cases = {\"No regularization (Overfitting) (Î» = 0)\": 0,\n",
    "         \"Training data with decision boundary (Î» = 1)\": 1,\n",
    "         \"Too much regularization (Underfitting) (Î» = 100)\": 100}\n",
    "\n",
    "for k, v in cases.items():\n",
    "    print(60 * \"-\")\n",
    "    print()\n",
    "    print(k)\n",
    "    \n",
    "    # Optimize the cost function.\n",
    "    res = minimize(fun=computeCost, x0=theta, args=(x, y, v))\n",
    "    theta_opt = res.x\n",
    "    \n",
    "    # Compute the accuracy.\n",
    "    predicted = predictAdmission(x, theta_opt)\n",
    "    correct = np.sum(predicted.astype(int) == y.ravel())\n",
    "    total = len(predicted)\n",
    "    print('Accuracy score: {}%'.format(round((100 * correct / total), 1)))\n",
    "    \n",
    "    # Reshape theta to (n, 1).\n",
    "    theta_opt = theta_opt.reshape(len(theta), 1)\n",
    "\n",
    "    # Create the meshgrid.\n",
    "    xs = np.linspace(-1, 1.5, 50)\n",
    "    ys = np.linspace(-1, 1.5, 50)\n",
    "    xx, yy = np.meshgrid(xs, ys)\n",
    "    # Note the input to np.zeros is a tuple!!!\n",
    "    zs = np.zeros((len(xs), len(ys)))\n",
    "\n",
    "    # Create the dataframe from the above columns.\n",
    "    data = pd.DataFrame({'xs': xx.ravel(),\n",
    "                         'ys': yy.ravel(),\n",
    "                         'zs': zs.ravel()})\n",
    "\n",
    "    # Transform the df.\n",
    "    transformed_data = mapFeature(data, 2, 6)\n",
    "\n",
    "    # Get the number of features.\n",
    "    n = len(transformed_data.columns)-1\n",
    "\n",
    "    # Prepare the data.\n",
    "    xx1, yy1, theta = prepareData(transformed_data, n)\n",
    "\n",
    "    # Compute the classifierâs predictions.\n",
    "    h_theta = sigmoid(np.dot(xx1, theta_opt))\n",
    "    h_theta = h_theta.reshape((50, 50))\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.xlabel('Microchip Test 1')\n",
    "    plt.ylabel('Microchip Test 2')\n",
    "    plt.title('Plot of training data')\n",
    "    plt.plot(data2['Test1'][data2['Accepted']==1],\n",
    "             data2['Test2'][data2['Accepted']==1], 'k+',\n",
    "             label='y = 1')\n",
    "    plt.plot(data2['Test1'][data2['Accepted']==0],\n",
    "             data2['Test2'][data2['Accepted']==0], 'yo',\n",
    "             label='y = 0')\n",
    "    plt.legend()\n",
    "    plt.contour(xx, yy, h_theta, [0.5])\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
